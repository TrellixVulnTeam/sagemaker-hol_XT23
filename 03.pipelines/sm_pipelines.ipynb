{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1858709d",
   "metadata": {},
   "source": [
    "# SageMaker Pipelines Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb471d",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7cae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e50741",
   "metadata": {},
   "source": [
    "## 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c5c1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket() \n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "os.makedirs(raw_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28cb1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 1us/step\n",
      "s3://sagemaker-ap-northeast-2-889750940888/sm-pipelines/data/raw\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "s3_prefix = 'sm-pipelines'\n",
    "rawdata_s3_prefix = '{}/data/raw'.format(s3_prefix)\n",
    "raw_s3 = sess.upload_data(path='./data/raw/', key_prefix=rawdata_s3_prefix)\n",
    "print(raw_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff64fb",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc8b2c",
   "metadata": {},
   "source": [
    "### Pipelines 변수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa87fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat\n",
    ")\n",
    "\n",
    "### ProcessingStep\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "processing_input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=raw_s3,\n",
    ")\n",
    "\n",
    "### TrainingStep\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "training_instance_count = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value= 1\n",
    ")\n",
    "\n",
    "# training_hp_epochs = ParameterInteger(\n",
    "training_hp_epochs = ParameterString(\n",
    "    name=\"TrainingHPEpochs\",\n",
    "    default_value= '5'\n",
    ")\n",
    "\n",
    "# training_hp_batch_size = ParameterInteger(\n",
    "training_hp_batch_size = ParameterString(\n",
    "    name=\"TrainingHPBatchSize\",\n",
    "    default_value= '128'\n",
    ")\n",
    "\n",
    "# training_hp_learning_rate = ParameterFloat(\n",
    "training_hp_learning_rate = ParameterString(\n",
    "    name=\"TrainingHPLearningRate\",\n",
    "    default_value= '0.01'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c9a34",
   "metadata": {},
   "source": [
    "### Caching config\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1927b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_config = sagemaker.workflow.steps.CacheConfig(enable_caching=True, expire_after=\"1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f4663",
   "metadata": {},
   "source": [
    "## ProcessingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a5c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "973d7535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data: \n",
      " s3://sagemaker-ap-northeast-2-889750940888/sm-pipelines/data/raw\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sm-pipelines\",\n",
    "    role=role,\n",
    ")\n",
    "print(\"input_data: \\n\", processing_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "786fe515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "output_destination = 's3://{}/{}/data'.format(bucket, s3_prefix)\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"Processing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=raw_s3,\n",
    "                        destination='/opt/ml/processing/input',\n",
    "                        s3_data_distribution_type='ShardedByS3Key')\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"train\",\n",
    "                              source='/opt/ml/processing/train',\n",
    "                              destination='{}/train'.format(output_destination)),\n",
    "             ProcessingOutput(output_name=\"test\",\n",
    "                              source='/opt/ml/processing/test',\n",
    "                              destination='{}/test'.format(output_destination))\n",
    "    ],\n",
    "#     job_arguments=[\"--split_rate\", f\"{split_rate}\"],    \n",
    "    code= 'preprocessing.py',\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10e32556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train/y_train.npy to s3://sagemaker-ap-northeast-2-889750940888/sm-pipelines/data/train/y_train.npy\n",
      "upload: data/test/y_test.npy to s3://sagemaker-ap-northeast-2-889750940888/sm-pipelines/data/test/y_test.npy\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {train_dir}/y_train.npy {output_destination}/train/y_train.npy\n",
    "!aws s3 cp {test_dir}/y_test.npy {output_destination}/test/y_test.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39e6bf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-10 03:34:37      10736 sm-pipelines/data/raw/x_test.npy\n",
      "2021-12-10 03:34:37      42144 sm-pipelines/data/raw/x_train.npy\n",
      "2021-12-10 03:57:45      10736 sm-pipelines/data/test/x_test.npy\n",
      "2021-12-10 04:05:10        944 sm-pipelines/data/test/y_test.npy\n",
      "2021-12-10 03:57:44      42144 sm-pipelines/data/train/x_train.npy\n",
      "2021-12-10 04:05:10       3360 sm-pipelines/data/train/y_train.npy\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {output_destination} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db267e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_process.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3f799",
   "metadata": {},
   "source": [
    "## TrainingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b45b607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hyperparameters = {'epochs': training_hp_epochs,\n",
    "                   'batch_size': training_hp_batch_size,\n",
    "                   'learning_rate': training_hp_learning_rate}\n",
    "\n",
    "metric_definitions = [{'Name': 'loss',\n",
    "                       'Regex': ' loss: ([0-9\\\\.]+)'},\n",
    "                     {'Name': 'val_loss',\n",
    "                       'Regex': ' val_loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "estimator = TensorFlow(source_dir='train_model',\n",
    "                       entry_point='train.py',\n",
    "#                       model_dir=model_dir,\n",
    "                       instance_type=training_instance_type,\n",
    "                       instance_count=training_instance_count,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       role=role,\n",
    "                       base_job_name='sm-pipelines',\n",
    "                       framework_version='2.1',\n",
    "                       py_version='py3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9834b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"Training\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data= step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data= step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    "#     cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c3b9d",
   "metadata": {},
   "source": [
    "## CreateModelStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "282a081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "    \n",
    "model = Model(\n",
    "    image_uri= step_train.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data= step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "284dc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    ")\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0557a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193db30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b3c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd4e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf652682",
   "metadata": {},
   "source": [
    "## Pipelines 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "920af7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = 'sm-pipelines-demo'\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        processing_input_data,\n",
    "        training_instance_type,\n",
    "        training_instance_count,\n",
    "        training_hp_epochs,\n",
    "        training_hp_batch_size,\n",
    "        training_hp_learning_rate\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_create_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bb3424",
   "metadata": {},
   "source": [
    "**파이프라인을 SageMaker에 제출하고 실행하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ede1f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-northeast-2:889750940888:pipeline/sm-pipelines-demo',\n",
       " 'ResponseMetadata': {'RequestId': '7fd864a2-45ef-454e-ad94-ff1e01ede244',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7fd864a2-45ef-454e-ad94-ff1e01ede244',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '90',\n",
       "   'date': 'Fri, 10 Dec 2021 04:05:12 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f9f2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        ProcessingInstanceType=\"ml.c5.xlarge\",\n",
    "        ProcessingInstanceCount=2,\n",
    "        TrainingHPEpochs=100\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b03c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95ca05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d635ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
