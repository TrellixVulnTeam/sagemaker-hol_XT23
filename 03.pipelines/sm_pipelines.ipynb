{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a2b25f",
   "metadata": {},
   "source": [
    "# SageMaker Pipelines Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5de81",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ba1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4232c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19805cc",
   "metadata": {},
   "source": [
    "## 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af39c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket() \n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "os.makedirs(raw_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87047766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-889750940888/sm-pipelines/data/raw\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "s3_prefix = 'sm-pipelines'\n",
    "rawdata_s3_prefix = '{}/data/raw'.format(s3_prefix)\n",
    "raw_s3 = sess.upload_data(path='./data/raw/', key_prefix=rawdata_s3_prefix)\n",
    "print(raw_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5f3cf",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098157a",
   "metadata": {},
   "source": [
    "### Pipelines 변수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17fdbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat\n",
    ")\n",
    "\n",
    "### ProcessingStep\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "processing_input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=raw_s3,\n",
    ")\n",
    "\n",
    "### TrainingStep\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "training_instance_count = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value= 1\n",
    ")\n",
    "\n",
    "# training_hp_epochs = ParameterInteger(\n",
    "training_hp_epochs = ParameterString(\n",
    "    name=\"TrainingHPEpochs\",\n",
    "    default_value= '5'\n",
    ")\n",
    "\n",
    "# training_hp_batch_size = ParameterInteger(\n",
    "training_hp_batch_size = ParameterString(\n",
    "    name=\"TrainingHPBatchSize\",\n",
    "    default_value= '128'\n",
    ")\n",
    "\n",
    "# training_hp_learning_rate = ParameterFloat(\n",
    "training_hp_learning_rate = ParameterString(\n",
    "    name=\"TrainingHPLearningRate\",\n",
    "    default_value= '0.01'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df04c2",
   "metadata": {},
   "source": [
    "### Caching config\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e14f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2bcbec",
   "metadata": {},
   "source": [
    "## ProcessingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88767c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67873f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sm-pipelines\",\n",
    "    role=role,\n",
    ")\n",
    "# print(\"input_data: \\n\", processing_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "345aaa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "output_destination = 's3://{}/{}/data'.format(bucket, s3_prefix)\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"Processing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=raw_s3,\n",
    "                        destination='/opt/ml/processing/input',\n",
    "                        s3_data_distribution_type='ShardedByS3Key')\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"train\",\n",
    "                              source='/opt/ml/processing/train',\n",
    "                              destination='{}/train'.format(output_destination)),\n",
    "             ProcessingOutput(output_name=\"test\",\n",
    "                              source='/opt/ml/processing/test',\n",
    "                              destination='{}/test'.format(output_destination))\n",
    "    ],\n",
    "#     job_arguments=[\"--split_rate\", f\"{split_rate}\"],    \n",
    "    code= 'preprocessing.py',\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8409ea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train/y_train.npy to s3://sagemaker-us-east-1-889750940888/sm-pipelines/data/train/y_train.npy\n",
      "upload: data/test/y_test.npy to s3://sagemaker-us-east-1-889750940888/sm-pipelines/data/test/y_test.npy\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {train_dir}/y_train.npy {output_destination}/train/y_train.npy\n",
    "!aws s3 cp {test_dir}/y_test.npy {output_destination}/test/y_test.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0b3553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-29 02:53:03      10736 sm-pipelines/data/raw/x_test.npy\n",
      "2022-03-29 02:53:03      42144 sm-pipelines/data/raw/x_train.npy\n",
      "2022-03-29 01:56:18      10736 sm-pipelines/data/test/x_test.npy\n",
      "2022-03-29 02:53:04        944 sm-pipelines/data/test/y_test.npy\n",
      "2022-03-29 01:56:18      42144 sm-pipelines/data/train/x_train.npy\n",
      "2022-03-29 02:53:03       3360 sm-pipelines/data/train/y_train.npy\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {output_destination} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1d8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_process.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a4bcca",
   "metadata": {},
   "source": [
    "## TuningStep\n",
    "- For 'A' customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bff048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    ContinuousParameter,\n",
    "    IntegerParameter,\n",
    "    HyperparameterTuner,\n",
    "    WarmStartConfig,\n",
    "    WarmStartTypes,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f246408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hyperparameters = {'epochs': training_hp_epochs,\n",
    "                   'batch_size': training_hp_batch_size,\n",
    "                   'learning_rate': training_hp_learning_rate}\n",
    "\n",
    "metric_definitions = [{'Name': 'loss',\n",
    "                       'Regex': ' loss: ([0-9\\\\.]+)'},\n",
    "                     {'Name': 'val_loss',\n",
    "                       'Regex': ' val_loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "estimator = TensorFlow(source_dir='train_model',\n",
    "                       entry_point='train.py',\n",
    "#                       model_dir=model_dir,\n",
    "                       instance_type=training_instance_type,\n",
    "                       instance_count=training_instance_count,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       role=role,\n",
    "                       base_job_name='sm-pipelines',\n",
    "                       framework_version='2.1',\n",
    "                       py_version='py3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc441bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n",
    "    \"batch_size\": IntegerParameter(4, 128, scaling_type=\"Linear\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b63d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_log = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name='val_loss',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=3,\n",
    "    strategy=\"Bayesian\",\n",
    "    objective_type=\"Minimize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04d5825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_tuning = TuningStep(\n",
    "    name=\"HPTuning\",\n",
    "    tuner=tuner_log,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data= step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data= step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "#             s3_data= Processing.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    "#     cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2da33",
   "metadata": {},
   "source": [
    "## TrainingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66f5cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hyperparameters = {'epochs': training_hp_epochs,\n",
    "                   'batch_size': step_tuning.properties.BestTrainingJob.TunedHyperParameters['batch_size'],\n",
    "                   'learning_rate': step_tuning.properties.BestTrainingJob.TunedHyperParameters['learning_rate']}\n",
    "\n",
    "\n",
    "metric_definitions = [{'Name': 'loss',\n",
    "                       'Regex': ' loss: ([0-9\\\\.]+)'},\n",
    "                     {'Name': 'val_loss',\n",
    "                       'Regex': ' val_loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "model_dir = '/opt/ml/model'\n",
    "\n",
    "estimator = TensorFlow(source_dir='train_model',\n",
    "                       entry_point='train.py',\n",
    "                      model_dir=model_dir,\n",
    "                       instance_type=training_instance_type,\n",
    "                       instance_count=training_instance_count,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       role=role,\n",
    "                       base_job_name='sm-pipelines',\n",
    "                       framework_version='2.1',\n",
    "                       py_version='py3',\n",
    "                       disable_profiler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8a8332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"Training\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data= step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data= step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e033584",
   "metadata": {},
   "source": [
    "## CreateModelStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa08ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b82309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1-cpu'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_image = retrieve(\n",
    "    framework = 'tensorflow',\n",
    "    region = boto3.session.Session().region_name,\n",
    "    version = '2.1',\n",
    "    py_version = 'py3',\n",
    "    instance_type = 'ml.c5.xlarge',\n",
    "    image_scope = 'inference'\n",
    ")\n",
    "inference_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e7d33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "    \n",
    "model = Model(\n",
    "#     image_uri= step_train.properties.AlgorithmSpecification.TrainingImage,\n",
    "    image_uri = inference_image,\n",
    "    model_data= step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a8de1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    ")\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b79c62b",
   "metadata": {},
   "source": [
    "## Pipelines 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0c3a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = 'sm-pipelines-demo'\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        processing_input_data,\n",
    "        training_instance_type,\n",
    "        training_instance_count,\n",
    "        training_hp_epochs,\n",
    "        training_hp_batch_size,\n",
    "        training_hp_learning_rate\n",
    "    ],\n",
    "#     steps=[step_process, step_train, step_tuning, step_create_model],\n",
    "    steps=[step_process, step_tuning, step_train, step_create_model],\n",
    "#     steps=[step_process, step_tuning, step_train],\n",
    "#     steps=[step_process, step_tuning]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7360901",
   "metadata": {},
   "source": [
    "**파이프라인을 SageMaker에 제출하고 실행하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce1139c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:889750940888:pipeline/sm-pipelines-demo',\n",
       " 'ResponseMetadata': {'RequestId': 'd10184cd-602b-45c4-a904-0202723e3039',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd10184cd-602b-45c4-a904-0202723e3039',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '85',\n",
       "   'date': 'Tue, 29 Mar 2022 03:07:59 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd3b7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        ProcessingInstanceType=\"ml.c5.xlarge\",\n",
    "        ProcessingInstanceCount=2,\n",
    "        TrainingHPEpochs=100\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed1f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5078513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
