{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d4d673",
   "metadata": {},
   "source": [
    "# TFServing on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909690ee",
   "metadata": {},
   "source": [
    "## 0. Create model artifacts w/ `inference.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30300dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n",
    "# may not use this file except in compliance with the License. A copy of\n",
    "# the License is located at\n",
    "#\n",
    "#     http://aws.amazon.com/apache2.0/\n",
    "#\n",
    "# or in the \"license\" file accompanying this file. This file is\n",
    "# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n",
    "# ANY KIND, either express or implied. See the License for the specific\n",
    "# language governing permissions and limitations under the License.\n",
    "\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def input_handler(data, context):\n",
    "    \"\"\"Pre-process request input before it is sent to TensorFlow Serving REST API\n",
    "\n",
    "    Args:\n",
    "        data (obj): the request data stream\n",
    "        context (Context): an object containing request and configuration details\n",
    "\n",
    "    Returns:\n",
    "        (dict): a JSON-serializable dict that contains request body and headers\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Start inference........................')\n",
    "\n",
    "    if context.request_content_type == \"application/json\":\n",
    "        payload = data.read()                       # bytes\n",
    "        decoded_payload = payload.decode('utf-8')   # Str\n",
    "\n",
    "        return decoded_payload\n",
    "    else:\n",
    "        _return_error(\n",
    "            415, 'Unsupported content type \"{}\"'.format(context.request_content_type or \"Unknown\")\n",
    "        )\n",
    "\n",
    "\n",
    "def output_handler(response, context):\n",
    "    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n",
    "\n",
    "    Args:\n",
    "        response (obj): the TensorFlow serving response\n",
    "        context (Context): an object containing request and configuration details\n",
    "\n",
    "    Returns:\n",
    "        (bytes, string): data to return to client, response content type\n",
    "    \"\"\"\n",
    "    if response.status_code != 200:\n",
    "        _return_error(response.status_code, response.content.decode(\"utf-8\"))\n",
    "    response_content_type = context.accept_header\n",
    "    prediction = response.content\n",
    "    return prediction, response_content_type\n",
    "\n",
    "\n",
    "def _return_error(code, message):\n",
    "    raise ValueError(\"Error: {}, {}\".format(str(code), message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3de2d6",
   "metadata": {},
   "source": [
    "### SavedModel의 input/output 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ca89f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['conv2d_input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 28, 28, 1)\n",
      "        name: serving_default_conv2d_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir model/1 --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e03d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/inference.py\n",
      "code/\n",
      "code/inference.py\n",
      "1/\n",
      "1/saved_model.pb\n",
      "1/assets/\n",
      "1/variables/\n",
      "1/variables/variables.index\n",
      "1/variables/variables.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "!tar -cvzf model.tar.gz code code --directory=model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78875f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-northeast-2-889750940888/sinjoonk/tfserving-sm/model/model.tar.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sinjoonk/tfserving-sm'\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sm_model = 's3://{}/{}/model/model.tar.gz'.format(bucket, prefix)\n",
    "sm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1334a1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-ap-northeast-2-889750940888/sinjoonk/tfserving-sm/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./model.tar.gz {sm_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceda534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658035c3",
   "metadata": {},
   "source": [
    "## 1. Create Tensorflow model object\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html?highlight=create%20model%20tensorflow#sagemaker.tensorflow.model.TensorFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3135faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9c4fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TensorFlowModel(\n",
    "    model_data = sm_model,\n",
    "    framework_version='2.0.0',\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8caec",
   "metadata": {},
   "source": [
    "## 2. Create Local endpoint using SM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5e9158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to ylfemq3668-algo-1-zcp29\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:starting services\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:using default model name: model\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:tensorflow serving model config: \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m model_config_list: {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   config: {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     name: \"model\",\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     base_path: \"/opt/ml/model\",\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     model_platform: \"tensorflow\"\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:nginx config: \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m load_module modules/ngx_http_js_module.so;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m worker_processes auto;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m daemon off;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m error_log  /dev/stderr error;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m events {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m http {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   default_type application/json;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   js_include tensorflow-serving.js;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   upstream tfs_upstream {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     server localhost:8501;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   upstream gunicorn_upstream {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     server unix:/tmp/gunicorn.sock fail_timeout=1;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   server {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     client_body_buffer_size 100m;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     subrequest_output_buffer_size 100m;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     set $tfs_version 2.0;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     set $default_tfs_model model;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     location /tfs {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         rewrite ^/tfs/(.*) /$1  break;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         proxy_redirect off;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         proxy_pass_request_headers off;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         proxy_set_header Content-Type 'application/json';\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         proxy_set_header Accept 'application/json';\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         proxy_pass http://tfs_upstream;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     location /ping {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         js_content ping_without_model;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     location /invocations {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         proxy_pass http://gunicorn_upstream/invocations;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     location ~ ^/models/(.*)/invoke {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         js_content invocations;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     location /models {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         proxy_pass http://gunicorn_upstream/models;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     location / {\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m         return 404 '{\"error\": \"Not Found\"}';\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m   }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:tensorflow version info:\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m TensorFlow ModelServer: 2.0.0+dev.sha.642edcd\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m TensorFlow Library: 2.0.0\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:started tensorflow serving (pid: 11)\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:gunicorn command: gunicorn -b unix:/tmp/gunicorn.sock -k gevent --chdir /sagemaker --pythonpath /opt/ml/model/code -e TFS_GRPC_PORT=9000 -e SAGEMAKER_MULTI_MODEL=False python_service:app\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.420825: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.420855: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: model\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.521134: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.521159: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 1}\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.521172: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 1}\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.521191: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 1}\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.521215: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/1\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:gunicorn version info:\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m gunicorn (version 20.0.4)\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:started gunicorn (pid: 31)\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.525316: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.529351: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.546691: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.579990: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.664608: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/1\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.674154: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 152938 microseconds.\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.675064: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /opt/ml/model/1/assets.extra/tf_serving_warmup_requests\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.675658: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.675679: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.677910: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 2021-12-30 05:50:36.679259: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m [evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m [2021-12-30 05:50:36 +0000] [31] [INFO] Starting gunicorn 20.0.4\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m [2021-12-30 05:50:36 +0000] [31] [INFO] Listening at: unix:/tmp/gunicorn.sock (31)\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:gunicorn server is ready!\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m [2021-12-30 05:50:36 +0000] [31] [INFO] Using worker: gevent\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m [2021-12-30 05:50:36 +0000] [64] [INFO] Booting worker with pid: 64\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:nginx version info:\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m nginx version: nginx/1.16.1\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m built by gcc 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) \n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m built with OpenSSL 1.1.1  11 Sep 2018\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m TLS SNI support enabled\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=/data/builder/debuild/nginx-1.16.1/debian/debuild-base/nginx-1.16.1=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\n",
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m INFO:__main__:started nginx (pid: 65)\n",
      "!\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 172.18.0.1 - - [30/Dec/2021:05:50:39 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"python-urllib3/1.26.7\"\n"
     ]
    }
   ],
   "source": [
    "local_ep = tf_model.deploy(\n",
    "    instance_type = 'local',\n",
    "    initial_instance_count = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2453d91",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace19ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb790752",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'0': 'T-shirt/top',\n",
    "          '1': 'Trouser',\n",
    "          '2': 'Pullover',\n",
    "          '3': 'Dress',\n",
    "          '4': 'Coat',\n",
    "          '5': 'Sandal',\n",
    "          '6': 'Shirt',\n",
    "          '7': 'Sneaker',\n",
    "          '8': 'Bag',\n",
    "          '9': 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ae4da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_mnist_show(n):\n",
    "    image = test_images[n]\n",
    "    image_reshaped = image.reshape(28, 28)\n",
    "    label = labels[str(test_labels[n])]\n",
    "    plt.figure(figsize = (2, 2))\n",
    "    plt.title(\"sample of \" + str(label))\n",
    "    plt.imshow(image_reshaped, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16218413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fashion_mnist_show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b58fc07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m Start inference........................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 172.18.0.1 - - [30/Dec/2021:05:50:41 +0000] \"POST /invocations HTTP/1.1\" 200 80 \"-\" \"python-urllib3/1.26.7\"\n"
     ]
    }
   ],
   "source": [
    "image = test_images[100]             # ndarray\n",
    "image = image.reshape(-1, 28, 28, 1) # ndarray\n",
    "payload = {\"signature_name\": \"serving_default\",\n",
    "           \"instances\": image.tolist()}\n",
    "\n",
    "local_ep.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258140d",
   "metadata": {},
   "source": [
    "### curl + POST로 추론 요청: Not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "746b7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_mnist_inference(n):\n",
    "    image = test_images[n]\n",
    "    image = image.reshape(-1, 28, 28, 1)\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    payload = json.dumps({\"signature_name\": \"serving_default\",\n",
    "                          \"instances\": image.tolist()})\n",
    "    response = requests.post('http://localhost:8501/v1/models/mnist:predict',\n",
    "                              data=payload, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a34b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion_mnist_inference(100) # ConnectionRefusedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d89afd",
   "metadata": {},
   "source": [
    "### endpoint.predict 사용: Not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d093da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m Start inference........................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mylfemq3668-algo-1-zcp29 |\u001b[0m 172.18.0.1 - - [30/Dec/2021:05:50:45 +0000] \"POST /invocations HTTP/1.1\" 200 80 \"-\" \"python-urllib3/1.26.7\"\n"
     ]
    }
   ],
   "source": [
    "image = test_images[100]             # ndarray\n",
    "image = image.reshape(-1, 28, 28, 1) # ndarray\n",
    "payload = {\"signature_name\": \"serving_default\",\n",
    "           \"instances\": image.tolist()}\n",
    "\n",
    "local_ep.predict(payload)            # Working\n",
    "# local_ep.predict(image.tolist())     # Not working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e05b65",
   "metadata": {},
   "source": [
    "### boto3 사용: Not working\n",
    "An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint tensorflow-inference-2021-12-30-04-55-52-733 of account 889750940888 not found. <- local endpoint로 invoke_endpoint는 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d49ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_runtime = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6cf3651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tensorflow-inference-2021-12-30-05-50-34-177'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_ep.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "862b1f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint tensorflow-inference-2021-12-30-05-50-34-177 of account 889750940888 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8281e76258de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_ep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mContentType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'application/json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#     Accept='string',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     CustomAttributes='string',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint tensorflow-inference-2021-12-30-05-50-34-177 of account 889750940888 not found."
     ]
    }
   ],
   "source": [
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=local_ep.endpoint,\n",
    "    Body=bytes(json.dumps(payload), 'utf-8'),\n",
    "    ContentType='application/json',\n",
    "#     Accept='string',\n",
    "#     CustomAttributes='string',\n",
    "#     TargetModel='string',\n",
    "#     TargetVariant='string',\n",
    "#     TargetContainerHostname='string',\n",
    "#     InferenceId='string'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c21f2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mylfemq3668-algo-1-zcp29 exited with code 137\n",
      "26aa6b6a5e69\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sagemaker/local/image.py\", line 837, in run\n",
      "    _stream_output(self.process)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sagemaker/local/image.py\", line 899, in _stream_output\n",
      "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
      "RuntimeError: Process exited with code: 137\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/sagemaker/local/image.py\", line 842, in run\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmpypme395z/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!docker kill $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf388e3",
   "metadata": {},
   "source": [
    "## 3. Create SM endpoint using SM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "895958b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TensorFlowModel(\n",
    "    model_data = sm_model,\n",
    "    framework_version='2.0.0',\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7051e361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------"
     ]
    }
   ],
   "source": [
    "sm_ep = tf_model.deploy(\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    initial_instance_count = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb59e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=sm_ep.endpoint,\n",
    "    Body=json.dumps(payload),\n",
    "#     Body=bytes(json.dumps(payload), 'utf-8'),\n",
    "#     Body=payload,\n",
    "    ContentType='application/json',\n",
    "#     Accept='string',\n",
    "#     CustomAttributes='string',\n",
    "#     TargetModel='string',\n",
    "#     TargetVariant='string',\n",
    "#     TargetContainerHostname='string',\n",
    "#     InferenceId='string'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ebf7ce",
   "metadata": {},
   "source": [
    "## 4. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_ep.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c133169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_latest_p37",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
