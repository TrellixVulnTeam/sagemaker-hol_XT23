{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cdb4b45",
   "metadata": {},
   "source": [
    "# TFServing on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df60317",
   "metadata": {},
   "source": [
    "## 0. Create model artifacts w/ `inference.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99ec5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "######################################################################################################\n",
    "# https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html\n",
    "######################################################################################################\n",
    "\n",
    "import json\n",
    "\n",
    "def input_handler(data, context):\n",
    "    \"\"\" Pre-process request input before it is sent to TensorFlow Serving REST API\n",
    "    Args:\n",
    "        data (obj): the request data, in format of dict or string\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (dict): a JSON-serializable dict that contains request body and headers\n",
    "    \"\"\"\n",
    "    if context.request_content_type == 'application/json':\n",
    "        print('Start inference.........')\n",
    "        # pass through json (assumes it's correctly formed)\n",
    "        d = data.read().decode('utf-8')\n",
    "        return d if len(d) else ''\n",
    "\n",
    "    if context.request_content_type == 'text/csv':\n",
    "        # very simple csv handler\n",
    "        return json.dumps({\n",
    "            'instances': [float(x) for x in data.read().decode('utf-8').split(',')]\n",
    "        })\n",
    "\n",
    "    raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(\n",
    "        context.request_content_type or \"unknown\"))\n",
    "\n",
    "\n",
    "def output_handler(data, context):\n",
    "    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n",
    "    Args:\n",
    "        data (obj): the TensorFlow serving response\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (bytes, string): data to return to client, response content type\n",
    "    \"\"\"\n",
    "    if data.status_code != 200:\n",
    "        raise ValueError(data.content.decode('utf-8'))\n",
    "\n",
    "    response_content_type = context.accept_header\n",
    "    prediction = data.content\n",
    "    return prediction, response_content_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586cd0d",
   "metadata": {},
   "source": [
    "### SavedModel의 input/output 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa8611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['conv2d_input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 28, 28, 1)\n",
      "        name: serving_default_conv2d_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir model/1 --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14bbdc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/inference.py\n",
      "1/\n",
      "1/saved_model.pb\n",
      "1/variables/\n",
      "1/variables/variables.data-00000-of-00001\n",
      "1/variables/variables.index\n"
     ]
    }
   ],
   "source": [
    "!tar -cvzf model.tar.gz code --directory=model 1\n",
    "# !tar -cvzf model.tar.gz --directory=model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb661fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-889750940888/sinjoonk/tfserving-sm/model/model.tar.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sinjoonk/tfserving-sm'\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sm_model = 's3://{}/{}/model/model.tar.gz'.format(bucket, prefix)\n",
    "sm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56fd7b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-889750940888/sinjoonk/tfserving-sm/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./model.tar.gz {sm_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f579237",
   "metadata": {},
   "source": [
    "## 1. Create Tensorflow model object\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html?highlight=create%20model%20tensorflow#sagemaker.tensorflow.model.TensorFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221a1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89cd3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TensorFlowModel(\n",
    "    model_data = sm_model,\n",
    "    framework_version='2.0.0',\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f75620",
   "metadata": {},
   "source": [
    "## 2. Create Local endpoint using SM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a71c89ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to hvbnk0j7vl-algo-1-x6psm\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:starting services\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:using default model name: model\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:tensorflow serving model config: \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m model_config_list: {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   config: {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     name: \"model\",\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     base_path: \"/opt/ml/model\",\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     model_platform: \"tensorflow\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:nginx config: \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m load_module modules/ngx_http_js_module.so;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m worker_processes auto;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m daemon off;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m error_log  /dev/stderr error;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m events {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m http {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   default_type application/json;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   js_include tensorflow-serving.js;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   upstream tfs_upstream {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     server localhost:8501;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   upstream gunicorn_upstream {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     server unix:/tmp/gunicorn.sock fail_timeout=1;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   server {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     client_body_buffer_size 100m;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     subrequest_output_buffer_size 100m;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     set $tfs_version 2.0;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     set $default_tfs_model model;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     location /tfs {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         rewrite ^/tfs/(.*) /$1  break;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         proxy_redirect off;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         proxy_pass_request_headers off;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         proxy_set_header Content-Type 'application/json';\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         proxy_set_header Accept 'application/json';\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         proxy_pass http://tfs_upstream;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     location /ping {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         js_content ping_without_model;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     location /invocations {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         proxy_pass http://gunicorn_upstream/invocations;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     location ~ ^/models/(.*)/invoke {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         js_content invocations;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     location /models {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         proxy_pass http://gunicorn_upstream/models;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     location / {\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m         return 404 '{\"error\": \"Not Found\"}';\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:tensorflow version info:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m TensorFlow ModelServer: 2.0.0+dev.sha.642edcd\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m TensorFlow Library: 2.0.0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:started tensorflow serving (pid: 12)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:gunicorn command: gunicorn -b unix:/tmp/gunicorn.sock -k gevent --chdir /sagemaker --pythonpath /opt/ml/model/code -e TFS_GRPC_PORT=9000 -e SAGEMAKER_MULTI_MODEL=False python_service:app\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:56.941047: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:56.941076: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: model\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.041328: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.041353: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.041364: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.041374: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.041401: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/1\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:gunicorn version info:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m gunicorn (version 20.0.4)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.044464: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:started gunicorn (pid: 32)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.048626: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.075043: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.112639: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [2022-01-18 15:51:57 +0000] [32] [INFO] Starting gunicorn 20.0.4\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:gunicorn server is ready!\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [2022-01-18 15:51:57 +0000] [32] [INFO] Listening at: unix:/tmp/gunicorn.sock (32)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [2022-01-18 15:51:57 +0000] [32] [INFO] Using worker: gevent\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [2022-01-18 15:51:57 +0000] [43] [INFO] Booting worker with pid: 43\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:nginx version info:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m nginx version: nginx/1.16.1\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m built by gcc 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m built with OpenSSL 1.1.1  11 Sep 2018\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m TLS SNI support enabled\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=/data/builder/debuild/nginx-1.16.1/debian/debuild-base/nginx-1.16.1=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:started nginx (pid: 44)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.229367: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/1\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.241254: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 199851 microseconds.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.242255: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /opt/ml/model/1/assets.extra/tf_serving_warmup_requests\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.242938: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.242962: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.245563: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:51:57.246695: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "!\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 172.18.0.1 - - [18/Jan/2022:15:51:59 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"python-urllib3/1.26.7\"\n"
     ]
    }
   ],
   "source": [
    "local_ep = tf_model.deploy(\n",
    "    instance_type = 'local',\n",
    "    initial_instance_count = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c4ce1",
   "metadata": {},
   "source": [
    "## 3. Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea4259de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "321c2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'0': 'T-shirt/top',\n",
    "          '1': 'Trouser',\n",
    "          '2': 'Pullover',\n",
    "          '3': 'Dress',\n",
    "          '4': 'Coat',\n",
    "          '5': 'Sandal',\n",
    "          '6': 'Shirt',\n",
    "          '7': 'Sneaker',\n",
    "          '8': 'Bag',\n",
    "          '9': 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34c66292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_mnist_show(n):\n",
    "    image = test_images[n]\n",
    "    image_reshaped = image.reshape(28, 28)\n",
    "    label = labels[str(test_labels[n])]\n",
    "    plt.figure(figsize = (2, 2))\n",
    "    plt.title(\"sample of \" + str(label))\n",
    "    plt.imshow(image_reshaped, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af75efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fashion_mnist_show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa5308",
   "metadata": {},
   "source": [
    "### curl + POST로 추론 요청: Not working (SigV4 인증 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b380d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_mnist_inference(n):\n",
    "    image = test_images[n]\n",
    "    image = image.reshape(-1, 28, 28, 1)\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    payload = json.dumps({\"signature_name\": \"serving_default\",\n",
    "                          \"instances\": image.tolist()})\n",
    "    response = requests.post('http://localhost:8501/v1/models/mnist:predict',\n",
    "                              data=payload, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39bacb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion_mnist_inference(100) # ConnectionRefusedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26107634",
   "metadata": {},
   "source": [
    "### endpoint.predict 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c6578da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Start inference.........\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.000643166,\n",
       "   6.31539321e-10,\n",
       "   2.11405e-05,\n",
       "   8.56643146e-06,\n",
       "   2.67335645e-06,\n",
       "   5.4633869e-08,\n",
       "   0.85118109,\n",
       "   2.62150536e-07,\n",
       "   0.148130327,\n",
       "   1.27195453e-05]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 172.18.0.1 - - [18/Jan/2022:15:53:33 +0000] \"POST /invocations HTTP/1.1\" 200 176 \"-\" \"python-urllib3/1.26.7\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Start inference.........\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.051341: F external/org_tensorflow/tensorflow/core/util/tensor_format.h:426] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m WARNING:__main__:unexpected tensorflow serving exit (status: 6). restarting.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m ERROR:python_service:exception handling request: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 672, in urlopen\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     chunked=chunked,\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 421, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     six.raise_from(e, None)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"<string>\", line 3, in raise_from\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 416, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     httplib_response = conn.getresponse()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 1346, in getresponse\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     response.begin()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 307, in begin\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     version, status, reason = self._read_status()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 276, in _read_status\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 449, in send\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     timeout=timeout\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 720, in urlopen\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\", line 400, in increment\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise six.reraise(type(error), error, _stacktrace)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py\", line 734, in reraise\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise value.with_traceback(tb)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 672, in urlopen\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     chunked=chunked,\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 421, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     six.raise_from(e, None)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"<string>\", line 3, in raise_from\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 416, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     httplib_response = conn.getresponse()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 1346, in getresponse\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     response.begin()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 307, in begin\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     version, status, reason = self._read_status()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 276, in _read_status\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/sagemaker/python_service.py\", line 63, in on_post\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     res.body, res.content_type = self._handlers(data, context)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/sagemaker/python_service.py\", line 93, in handler\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     response = requests.post(context.rest_uri, data=processed_input)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 116, in post\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     return request('post', url, data=data, json=json, **kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 60, in request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     return session.request(method=method, url=url, **kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 533, in request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     resp = self.send(prep, **send_kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 646, in send\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     r = adapter.send(request, **kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 498, in send\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise ConnectionError(err, request=request)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 172.18.0.1 - - [18/Jan/2022:15:54:19 +0000] \"POST /invocations HTTP/1.1\" 500 106 \"-\" \"python-urllib3/1.26.7\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:tensorflow version info:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m TensorFlow ModelServer: 2.0.0+dev.sha.642edcd\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m TensorFlow Library: 2.0.0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:started tensorflow serving (pid: 84)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.125172: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.125198: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: model\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.225429: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.225452: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.225464: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.225478: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.225530: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/1\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.228294: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.231193: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.231980: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.255410: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.340330: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/1\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.349307: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 123777 microseconds.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.349982: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /opt/ml/model/1/assets.extra/tf_serving_warmup_requests\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.350469: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.350487: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.352131: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:19.353053: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 172.18.0.1 - - [18/Jan/2022:15:54:24 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"python-urllib3/1.26.7\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Start inference.........\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.247807: F external/org_tensorflow/tensorflow/core/util/tensor_format.h:426] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m WARNING:__main__:unexpected tensorflow serving exit (status: 6). restarting.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m ERROR:python_service:exception handling request: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 672, in urlopen\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     chunked=chunked,\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 421, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     six.raise_from(e, None)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"<string>\", line 3, in raise_from\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 416, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     httplib_response = conn.getresponse()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 1346, in getresponse\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     response.begin()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 307, in begin\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     version, status, reason = self._read_status()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 276, in _read_status\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 449, in send\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     timeout=timeout\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 720, in urlopen\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\", line 400, in increment\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise six.reraise(type(error), error, _stacktrace)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py\", line 734, in reraise\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise value.with_traceback(tb)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 672, in urlopen\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     chunked=chunked,\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 421, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     six.raise_from(e, None)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"<string>\", line 3, in raise_from\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 416, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     httplib_response = conn.getresponse()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 1346, in getresponse\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     response.begin()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 307, in begin\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     version, status, reason = self._read_status()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 276, in _read_status\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/sagemaker/python_service.py\", line 63, in on_post\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     res.body, res.content_type = self._handlers(data, context)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/sagemaker/python_service.py\", line 93, in handler\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     response = requests.post(context.rest_uri, data=processed_input)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 116, in post\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     return request('post', url, data=data, json=json, **kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 60, in request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     return session.request(method=method, url=url, **kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 533, in request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     resp = self.send(prep, **send_kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 646, in send\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     r = adapter.send(request, **kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 498, in send\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise ConnectionError(err, request=request)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 172.18.0.1 - - [18/Jan/2022:15:54:30 +0000] \"POST /invocations HTTP/1.1\" 500 106 \"-\" \"python-urllib3/1.26.7\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:tensorflow version info:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m TensorFlow ModelServer: 2.0.0+dev.sha.642edcd\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m TensorFlow Library: 2.0.0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:started tensorflow serving (pid: 133)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.322304: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.322332: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: model\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.422569: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.422593: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.422603: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.422616: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.422672: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/1\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.425362: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.428201: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.428937: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.451786: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.535948: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/1\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.544874: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 122202 microseconds.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.545539: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /opt/ml/model/1/assets.extra/tf_serving_warmup_requests\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.545988: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.546006: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.547771: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:30.548676: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Start inference.........\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.166388: F external/org_tensorflow/tensorflow/core/util/tensor_format.h:426] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m WARNING:__main__:unexpected tensorflow serving exit (status: 6). restarting.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m ERROR:python_service:exception handling request: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 672, in urlopen\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     chunked=chunked,\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 421, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     six.raise_from(e, None)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"<string>\", line 3, in raise_from\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 416, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     httplib_response = conn.getresponse()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 1346, in getresponse\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     response.begin()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 307, in begin\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     version, status, reason = self._read_status()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 276, in _read_status\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 449, in send\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     timeout=timeout\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 720, in urlopen\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\", line 400, in increment\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise six.reraise(type(error), error, _stacktrace)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py\", line 734, in reraise\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise value.with_traceback(tb)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 672, in urlopen\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     chunked=chunked,\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 421, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     six.raise_from(e, None)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"<string>\", line 3, in raise_from\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 416, in _make_request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     httplib_response = conn.getresponse()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 1346, in getresponse\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     response.begin()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 307, in begin\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     version, status, reason = self._read_status()\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/lib/python3.6/http/client.py\", line 276, in _read_status\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/sagemaker/python_service.py\", line 63, in on_post\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     res.body, res.content_type = self._handlers(data, context)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/sagemaker/python_service.py\", line 93, in handler\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     response = requests.post(context.rest_uri, data=processed_input)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 116, in post\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     return request('post', url, data=data, json=json, **kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 60, in request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     return session.request(method=method, url=url, **kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 533, in request\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     resp = self.send(prep, **send_kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 646, in send\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     r = adapter.send(request, **kwargs)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 498, in send\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m     raise ConnectionError(err, request=request)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 172.18.0.1 - - [18/Jan/2022:15:54:33 +0000] \"POST /invocations HTTP/1.1\" 500 106 \"-\" \"python-urllib3/1.26.7\"\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:tensorflow version info:\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m TensorFlow ModelServer: 2.0.0+dev.sha.642edcd\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m TensorFlow Library: 2.0.0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m INFO:__main__:started tensorflow serving (pid: 182)\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.239005: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.239035: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: model\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.339274: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.339298: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.339308: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.339321: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.339346: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/1\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.342208: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.345061: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.345850: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.368541: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.450842: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/1\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.459338: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 119992 microseconds.\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.459969: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /opt/ml/model/1/assets.extra/tf_serving_warmup_requests\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.460414: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.460431: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.462136: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m 2022-01-18 15:54:33.463050: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm |\u001b[0m [evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "\u001b[36mhvbnk0j7vl-algo-1-x6psm exited with code 137\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 837, in run\n",
      "    _stream_output(self.process)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 899, in _stream_output\n",
      "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
      "RuntimeError: Process exited with code: 137\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 842, in run\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmppzx_c223/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "image = test_images[100]             # ndarray\n",
    "image = image.reshape(-1, 28, 28, 1) # ndarray\n",
    "\n",
    "arr = np.random.randint(2, size=28*28).reshape(-1, 28, 28, 1)\n",
    "\n",
    "payload = {\"signature_name\": \"serving_default\",\n",
    "           \"instances\": arr.tolist()}\n",
    "\n",
    "local_ep.predict(payload)            # Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1d78162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mbavb5sf870-algo-1-9iu8b exited with code 137\n",
      "4755483a9852\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 837, in run\n",
      "    _stream_output(self.process)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 899, in _stream_output\n",
      "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
      "RuntimeError: Process exited with code: 137\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 842, in run\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmp_7su_62f/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!docker kill $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80533af6",
   "metadata": {},
   "source": [
    "## 4. Create SM endpoint using SM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c17d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TensorFlowModel(\n",
    "    model_data = sm_model,\n",
    "    framework_version='2.0.0',\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85be5e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "sm_ep = tf_model.deploy(\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    initial_instance_count = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d33d1450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow-inference-2022-01-10-13-13-02-960'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_ep.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8307cf",
   "metadata": {},
   "source": [
    "### Inference by `invoke_endpoint`\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/neo-requests-boto3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cad48729",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_images[100]             # ndarray\n",
    "image = image.reshape(-1, 28, 28, 1) # ndarray\n",
    "payload = {\"signature_name\": \"serving_default\",\n",
    "           \"instances\": image.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de8c7a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=sm_ep.endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39654d03",
   "metadata": {},
   "source": [
    "## 5. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_ep.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d2ab6",
   "metadata": {},
   "source": [
    "# Serverless Inference (Preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78119e",
   "metadata": {},
   "source": [
    "### 1. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21b16729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import boto3\n",
    "import sagemaker\n",
    "region = boto3.Session().region_name\n",
    "client = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "#Role to give SageMaker permission to access AWS services.\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "#Get model from S3\n",
    "model_url = sm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16c23483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.0.0-cpu'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import image_uris\n",
    "container = image_uris.retrieve(\n",
    "    framework = 'tensorflow',\n",
    "    region = region,\n",
    "    version = '2.0.0',\n",
    "    py_version = 'py3',\n",
    "    image_scope = 'inference',\n",
    "    instance_type='ml.m5.xlarge'\n",
    ")\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ece1e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelArn': 'arn:aws:sagemaker:us-east-1:889750940888:model/serverless-inference-model',\n",
       " 'ResponseMetadata': {'RequestId': '310a29ef-bd8c-419c-a44e-d78a87a9de7d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '310a29ef-bd8c-419c-a44e-d78a87a9de7d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '88',\n",
       "   'date': 'Mon, 10 Jan 2022 13:21:54 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create model\n",
    "model_name = \"serverless-inference-model\"\n",
    "\n",
    "response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = sagemaker_role,\n",
    "    Containers = [{\n",
    "        \"Image\": container,\n",
    "        \"Mode\": \"SingleModel\",\n",
    "        \"ModelDataUrl\": model_url,\n",
    "    }]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7a4ec",
   "metadata": {},
   "source": [
    "## 2. Create Endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fcdff03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:889750940888:endpoint-config/serverless-inference-model-epc',\n",
       " 'ResponseMetadata': {'RequestId': '78d9e3b9-5b2a-43e7-a926-a634d38ac835',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '78d9e3b9-5b2a-43e7-a926-a634d38ac835',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '111',\n",
       "   'date': 'Mon, 10 Jan 2022 13:22:21 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.create_endpoint_config(\n",
    "   EndpointConfigName=\"serverless-inference-model-epc\",\n",
    "   ProductionVariants=[\n",
    "        {\n",
    "            \"ModelName\": \"serverless-inference-model\",\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ServerlessConfig\": {\n",
    "                \"MemorySizeInMB\": 2048,\n",
    "                \"MaxConcurrency\": 20\n",
    "            }\n",
    "        } \n",
    "    ]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a4f77",
   "metadata": {},
   "source": [
    "## 3. Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85442c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.create_endpoint(\n",
    "    EndpointName=\"serverless-inference-model-ep\",\n",
    "    EndpointConfigName=\"serverless-inference-model-epc\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3f085",
   "metadata": {},
   "source": [
    "## Invoke Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4865187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_images[100]             # ndarray\n",
    "image = image.reshape(-1, 28, 28, 1) # ndarray\n",
    "payload = {\"signature_name\": \"serving_default\",\n",
    "           \"instances\": image.tolist()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397b44a",
   "metadata": {},
   "source": [
    "#### Slow response due to cold start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "927422c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.85 ms, sys: 8.29 ms, total: 15.1 ms\n",
      "Wall time: 2.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=\"serverless-inference-model-ep\",\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7476a2c7",
   "metadata": {},
   "source": [
    "#### Fast response after 2nd invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f2eb5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 ms, sys: 0 ns, total: 15.1 ms\n",
      "Wall time: 267 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=\"serverless-inference-model-ep\",\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed2ab68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 ms, sys: 0 ns, total: 14.7 ms\n",
      "Wall time: 175 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=\"serverless-inference-model-ep\",\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9948a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
