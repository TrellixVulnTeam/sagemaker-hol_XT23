{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95b2020",
   "metadata": {},
   "source": [
    "# TFServing on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274923f2",
   "metadata": {},
   "source": [
    "## 0. Create model artifacts w/ `inference.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa2a1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "######################################################################################################\n",
    "# https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html\n",
    "######################################################################################################\n",
    "\n",
    "import json\n",
    "\n",
    "def input_handler(data, context):\n",
    "    \"\"\" Pre-process request input before it is sent to TensorFlow Serving REST API\n",
    "    Args:\n",
    "        data (obj): the request data, in format of dict or string\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (dict): a JSON-serializable dict that contains request body and headers\n",
    "    \"\"\"\n",
    "    if context.request_content_type == 'application/json':\n",
    "        print('Start inference.........')\n",
    "        # pass through json (assumes it's correctly formed)\n",
    "        d = data.read().decode('utf-8')\n",
    "        return d if len(d) else ''\n",
    "\n",
    "    if context.request_content_type == 'text/csv':\n",
    "        # very simple csv handler\n",
    "        return json.dumps({\n",
    "            'instances': [float(x) for x in data.read().decode('utf-8').split(',')]\n",
    "        })\n",
    "\n",
    "    raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(\n",
    "        context.request_content_type or \"unknown\"))\n",
    "\n",
    "\n",
    "def output_handler(data, context):\n",
    "    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n",
    "    Args:\n",
    "        data (obj): the TensorFlow serving response\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (bytes, string): data to return to client, response content type\n",
    "    \"\"\"\n",
    "    if data.status_code != 200:\n",
    "        raise ValueError(data.content.decode('utf-8'))\n",
    "\n",
    "    response_content_type = context.accept_header\n",
    "    prediction = data.content\n",
    "    return prediction, response_content_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80430b6d",
   "metadata": {},
   "source": [
    "### SavedModel의 input/output 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99be8604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['conv2d_input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 28, 28, 1)\n",
      "        name: serving_default_conv2d_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir model/1 --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ade8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/inference.py\n",
      "1/\n",
      "1/saved_model.pb\n",
      "1/variables/\n",
      "1/variables/variables.data-00000-of-00001\n",
      "1/variables/variables.index\n"
     ]
    }
   ],
   "source": [
    "!tar -cvzf model.tar.gz code --directory=model 1\n",
    "# !tar -cvzf model.tar.gz --directory=model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72d5b84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-889750940888/sinjoonk/tfserving-sm/model/model.tar.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sinjoonk/tfserving-sm'\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sm_model = 's3://{}/{}/model/model.tar.gz'.format(bucket, prefix)\n",
    "sm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8177d9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-889750940888/sinjoonk/tfserving-sm/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./model.tar.gz {sm_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e156e0",
   "metadata": {},
   "source": [
    "## 1. Create Tensorflow model object\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html?highlight=create%20model%20tensorflow#sagemaker.tensorflow.model.TensorFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "452835de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c98ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TensorFlowModel(\n",
    "    model_data = sm_model,\n",
    "    framework_version='2.0.0',\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89752846",
   "metadata": {},
   "source": [
    "## 2. Create Local endpoint using SM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d45488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to bavb5sf870-algo-1-9iu8b\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:starting services\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:using default model name: model\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:tensorflow serving model config: \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m model_config_list: {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   config: {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     name: \"model\",\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     base_path: \"/opt/ml/model\",\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     model_platform: \"tensorflow\"\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:nginx config: \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m load_module modules/ngx_http_js_module.so;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m worker_processes auto;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m daemon off;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m error_log  /dev/stderr error;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m events {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m http {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   default_type application/json;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   js_include tensorflow-serving.js;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   upstream tfs_upstream {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     server localhost:8501;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   upstream gunicorn_upstream {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     server unix:/tmp/gunicorn.sock fail_timeout=1;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   server {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     client_body_buffer_size 100m;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     subrequest_output_buffer_size 100m;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     set $tfs_version 2.0;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     set $default_tfs_model model;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     location /tfs {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         rewrite ^/tfs/(.*) /$1  break;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         proxy_redirect off;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         proxy_pass_request_headers off;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         proxy_set_header Content-Type 'application/json';\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         proxy_set_header Accept 'application/json';\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         proxy_pass http://tfs_upstream;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     location /ping {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         js_content ping_without_model;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     location /invocations {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         proxy_pass http://gunicorn_upstream/invocations;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     location ~ ^/models/(.*)/invoke {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         js_content invocations;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     location /models {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         proxy_pass http://gunicorn_upstream/models;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     location / {\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m         return 404 '{\"error\": \"Not Found\"}';\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m   }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:tensorflow version info:\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m TensorFlow ModelServer: 2.0.0+dev.sha.642edcd\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m TensorFlow Library: 2.0.0\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:started tensorflow serving (pid: 12)\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:gunicorn command: gunicorn -b unix:/tmp/gunicorn.sock -k gevent --chdir /sagemaker --pythonpath /opt/ml/model/code -e TFS_GRPC_PORT=9000 -e SAGEMAKER_MULTI_MODEL=False python_service:app\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:46.985230: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:46.985260: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: model\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:gunicorn version info:\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m gunicorn (version 20.0.4)\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.085620: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.085641: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 1}\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.085651: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 1}\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.085661: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 1}\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.085683: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/1\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:started gunicorn (pid: 32)\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.089627: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.095807: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.105590: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.140105: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.256694: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/1\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.271488: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 185803 microseconds.\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.272521: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /opt/ml/model/1/assets.extra/tf_serving_warmup_requests\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.273022: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: model version: 1} exhausted max_num_retries: 0\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.273040: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.276005: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 2022-01-10 13:12:47.277309: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m [evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m [2022-01-10 13:12:47 +0000] [32] [INFO] Starting gunicorn 20.0.4\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m [2022-01-10 13:12:47 +0000] [32] [INFO] Listening at: unix:/tmp/gunicorn.sock (32)\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:gunicorn server is ready!\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m [2022-01-10 13:12:47 +0000] [32] [INFO] Using worker: gevent\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m [2022-01-10 13:12:47 +0000] [65] [INFO] Booting worker with pid: 65\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:nginx version info:\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m nginx version: nginx/1.16.1\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m built by gcc 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) \n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m built with OpenSSL 1.1.1  11 Sep 2018\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m TLS SNI support enabled\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=/data/builder/debuild/nginx-1.16.1/debian/debuild-base/nginx-1.16.1=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\n",
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m INFO:__main__:started nginx (pid: 66)\n",
      "!\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 172.18.0.1 - - [10/Jan/2022:13:12:50 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"python-urllib3/1.26.7\"\n"
     ]
    }
   ],
   "source": [
    "local_ep = tf_model.deploy(\n",
    "    instance_type = 'local',\n",
    "    initial_instance_count = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c08e5",
   "metadata": {},
   "source": [
    "## 3. Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aeb9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e822b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'0': 'T-shirt/top',\n",
    "          '1': 'Trouser',\n",
    "          '2': 'Pullover',\n",
    "          '3': 'Dress',\n",
    "          '4': 'Coat',\n",
    "          '5': 'Sandal',\n",
    "          '6': 'Shirt',\n",
    "          '7': 'Sneaker',\n",
    "          '8': 'Bag',\n",
    "          '9': 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52bd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_mnist_show(n):\n",
    "    image = test_images[n]\n",
    "    image_reshaped = image.reshape(28, 28)\n",
    "    label = labels[str(test_labels[n])]\n",
    "    plt.figure(figsize = (2, 2))\n",
    "    plt.title(\"sample of \" + str(label))\n",
    "    plt.imshow(image_reshaped, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a47a59a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fashion_mnist_show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90eac0",
   "metadata": {},
   "source": [
    "### curl + POST로 추론 요청: Not working (SigV4 인증 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c728b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_mnist_inference(n):\n",
    "    image = test_images[n]\n",
    "    image = image.reshape(-1, 28, 28, 1)\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    payload = json.dumps({\"signature_name\": \"serving_default\",\n",
    "                          \"instances\": image.tolist()})\n",
    "    response = requests.post('http://localhost:8501/v1/models/mnist:predict',\n",
    "                              data=payload, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3584926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion_mnist_inference(100) # ConnectionRefusedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee50fb2",
   "metadata": {},
   "source": [
    "### endpoint.predict 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac84d5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m Start inference.........\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mbavb5sf870-algo-1-9iu8b |\u001b[0m 172.18.0.1 - - [10/Jan/2022:13:12:51 +0000] \"POST /invocations HTTP/1.1\" 200 80 \"-\" \"python-urllib3/1.26.7\"\n"
     ]
    }
   ],
   "source": [
    "image = test_images[100]             # ndarray\n",
    "image = image.reshape(-1, 28, 28, 1) # ndarray\n",
    "payload = {\"signature_name\": \"serving_default\",\n",
    "           \"instances\": image.tolist()}\n",
    "\n",
    "local_ep.predict(payload)            # Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da01ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mbavb5sf870-algo-1-9iu8b exited with code 137\n",
      "4755483a9852\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 837, in run\n",
      "    _stream_output(self.process)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 899, in _stream_output\n",
      "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
      "RuntimeError: Process exited with code: 137\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 842, in run\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmp_7su_62f/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!docker kill $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab300a43",
   "metadata": {},
   "source": [
    "## 4. Create SM endpoint using SM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c144fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TensorFlowModel(\n",
    "    model_data = sm_model,\n",
    "    framework_version='2.0.0',\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07240977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "sm_ep = tf_model.deploy(\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    initial_instance_count = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f8970c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow-inference-2022-01-10-13-13-02-960'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_ep.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906bfc3",
   "metadata": {},
   "source": [
    "### Inference by `invoke_endpoint`\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/neo-requests-boto3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2947ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_images[100]             # ndarray\n",
    "image = image.reshape(-1, 28, 28, 1) # ndarray\n",
    "payload = {\"signature_name\": \"serving_default\",\n",
    "           \"instances\": image.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1235d919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=sm_ep.endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a44673",
   "metadata": {},
   "source": [
    "## 5. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_ep.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efeed8",
   "metadata": {},
   "source": [
    "# Serverless Inference (Preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197aa9b",
   "metadata": {},
   "source": [
    "### 1. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "463182a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import boto3\n",
    "import sagemaker\n",
    "region = boto3.Session().region_name\n",
    "client = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "#Role to give SageMaker permission to access AWS services.\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "#Get model from S3\n",
    "model_url = sm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "103b8137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.0.0-cpu'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import image_uris\n",
    "container = image_uris.retrieve(\n",
    "    framework = 'tensorflow',\n",
    "    region = region,\n",
    "    version = '2.0.0',\n",
    "    py_version = 'py3',\n",
    "    image_scope = 'inference',\n",
    "    instance_type='ml.m5.xlarge'\n",
    ")\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f40d207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelArn': 'arn:aws:sagemaker:us-east-1:889750940888:model/serverless-inference-model',\n",
       " 'ResponseMetadata': {'RequestId': '310a29ef-bd8c-419c-a44e-d78a87a9de7d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '310a29ef-bd8c-419c-a44e-d78a87a9de7d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '88',\n",
       "   'date': 'Mon, 10 Jan 2022 13:21:54 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create model\n",
    "model_name = \"serverless-inference-model\"\n",
    "\n",
    "response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = sagemaker_role,\n",
    "    Containers = [{\n",
    "        \"Image\": container,\n",
    "        \"Mode\": \"SingleModel\",\n",
    "        \"ModelDataUrl\": model_url,\n",
    "    }]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d898ecf7",
   "metadata": {},
   "source": [
    "## 2. Create Endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdb811a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:889750940888:endpoint-config/serverless-inference-model-epc',\n",
       " 'ResponseMetadata': {'RequestId': '78d9e3b9-5b2a-43e7-a926-a634d38ac835',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '78d9e3b9-5b2a-43e7-a926-a634d38ac835',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '111',\n",
       "   'date': 'Mon, 10 Jan 2022 13:22:21 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.create_endpoint_config(\n",
    "   EndpointConfigName=\"serverless-inference-model-epc\",\n",
    "   ProductionVariants=[\n",
    "        {\n",
    "            \"ModelName\": \"serverless-inference-model\",\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ServerlessConfig\": {\n",
    "                \"MemorySizeInMB\": 2048,\n",
    "                \"MaxConcurrency\": 20\n",
    "            }\n",
    "        } \n",
    "    ]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da324c34",
   "metadata": {},
   "source": [
    "## 3. Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49fa8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.create_endpoint(\n",
    "    EndpointName=\"serverless-inference-model-ep\",\n",
    "    EndpointConfigName=\"serverless-inference-model-epc\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7271",
   "metadata": {},
   "source": [
    "## Invoke Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "147cbc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_images[100]             # ndarray\n",
    "image = image.reshape(-1, 28, 28, 1) # ndarray\n",
    "payload = {\"signature_name\": \"serving_default\",\n",
    "           \"instances\": image.tolist()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e26dc9f",
   "metadata": {},
   "source": [
    "#### Slow response due to cold start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89ff02e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.85 ms, sys: 8.29 ms, total: 15.1 ms\n",
      "Wall time: 2.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=\"serverless-inference-model-ep\",\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5a058",
   "metadata": {},
   "source": [
    "#### Fast response after 2nd invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67c49d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 ms, sys: 0 ns, total: 15.1 ms\n",
      "Wall time: 267 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=\"serverless-inference-model-ep\",\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42fd6af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 ms, sys: 0 ns, total: 14.7 ms\n",
      "Wall time: 175 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=\"serverless-inference-model-ep\",\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa34d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
